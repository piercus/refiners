script = "finetune-ldm.py" # not used for now

[wandb]
mode = "offline"
entity = "acme"
project = "test-ldm-training"

[models]
unet = {checkpoint = "tests/weights/stable-diffusion-1-5/unet.safetensors"}
text_encoder = {checkpoint = "tests/weights/stable-diffusion-1-5/CLIPTextEncoderL.safetensors"}
lda = {checkpoint = "tests/weights/stable-diffusion-1-5/lda.safetensors"}

[latent_diffusion]
unconditional_sampling_probability = 0.2
offset_noise = 0.1

[training]
duration = "1:epoch"
seed = 0
gpu_index = 0
num_epochs = 1
batch_size = 1
gradient_accumulation = "1:step"
clip_grad_norm = 2.0
clip_grad_value = 1.0
evaluation_interval = "1:epoch"
evaluation_seed = 0


[optimizer]
optimizer = "AdamW" # "AdamW", "AdamW8bit", "Lion8bit", "Prodigy", "SGD", "Adam"
learning_rate = 1e-5
betas = [0.9, 0.999]
eps = 1e-8
weight_decay = 1e-2


[scheduler]


[dropout]
dropout_probability = 0.2

[dataset]
hf_repo = "1aurent/unsplash-lite-palette"
revision = "main"
caption_key = "ai_description"

[checkpointing]
# save_folder = "/path/to/ckpts"
save_interval = "1:epoch"

[test_diffusion]
prompts = [
    "A cute cat",
]
